{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e80bb6f0-bac1-4a3c-a107-f54b4023a378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b072e852-c8ba-4a99-91d7-27b385ab0c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_recommenders as tfrs\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import StringLookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c644d273-188a-4278-a8c8-5c53e6681ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "product = pd.read_csv('product3.csv')\n",
    "\n",
    "user = pd.read_csv('user3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a2351631-8ea2-4a4b-8586-2a4596b22c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow datasets from the data\n",
    "user = tf.data.Dataset.from_tensor_slices({\n",
    "    \"user_id\": tf.strings.as_string(user['UserID'].values),  # Ensure user_id is string\n",
    "    \"product_title\": user['Title'].values,\n",
    "    \"interaction_type\": tf.cast(user['InteractionType'].values, tf.int32)\n",
    "})\n",
    "\n",
    "product = tf.data.Dataset.from_tensor_slices({\n",
    "    \"title\": product['Title'].values,\n",
    "    \"ingredients\": product['Ingredients'].values,\n",
    "    \"combined\": product['Combined'].values\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b9f5fcb8-1ec7-4c14-adf5-a0e863e7017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map function to format the user dataset\n",
    "user = user.map(lambda x: {\n",
    "    \"user_id\": tf.cast(x[\"user_id\"], tf.string),  # Ensure user_id is cast to string\n",
    "    \"product_title\": x[\"product_title\"],\n",
    "    \"interaction_type\": tf.cast(x[\"interaction_type\"], tf.int32)\n",
    "})\n",
    "\n",
    "# Map function to format the product dataset\n",
    "product = product.map(lambda x: {\n",
    "    \"title\": x[\"title\"],\n",
    "    \"ingredients\": x[\"ingredients\"],\n",
    "    \"combined\": x[\"combined\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "07582628-2931-439c-aba3-97339300a78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'     rendang ayam kacang merah' b'    ayam penyet'\n",
      " b'    kering balado tempe kacang pedas manis'\n",
      " b'    lele goreng krenyes no amis' b'    orek tahu warna warni'\n",
      " b'   gulai ikan kakap merah khas padang' b'   pesmol ikan kerapu'\n",
      " b'   telur dadar kelapa parut' b'  soto bening daging sapi'\n",
      " b'  tahu brokoli pedas manis']\n",
      "[b'1' b'10' b'11' b'12' b'13' b'14' b'15' b'16' b'17' b'18']\n"
     ]
    }
   ],
   "source": [
    "# Shuffle and split the user dataset\n",
    "shuffled_user = user.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "# Split into train and test sets\n",
    "train_user = shuffled_user.take(80_000)\n",
    "test_user = shuffled_user.skip(80_000).take(20_000)\n",
    "\n",
    "# Batch the product dataset\n",
    "batched_product = product.batch(1_000)\n",
    "\n",
    "# Extract unique movie/product titles\n",
    "product_titles = batched_product.map(lambda x: x[\"title\"])\n",
    "product_titles_list = list(product_titles)  # Extract the list of product titles\n",
    "unique_product_titles = np.unique(np.concatenate(product_titles_list))  # Concatenate and get unique titles\n",
    "\n",
    "# Extract unique user IDs\n",
    "user_ids = user.map(lambda x: x[\"user_id\"]).batch(1_000_000)\n",
    "user_ids_list = list(user_ids)  # Extract the list of user IDs\n",
    "unique_user_ids = np.unique(np.concatenate(user_ids_list))  # Concatenate and get unique IDs\n",
    "\n",
    "# Menampilkan contoh produk dan user\n",
    "print(unique_product_titles[:10])\n",
    "print(unique_user_ids[:10])\n",
    "\n",
    "# Membuat pasangan data (user_id, product_title)\n",
    "user_data = tf.data.Dataset.from_tensor_slices(unique_user_ids)\n",
    "product_data = tf.data.Dataset.from_tensor_slices(unique_product_titles)\n",
    "\n",
    "# Gabungkan kedua dataset (user, product) menjadi pasangan\n",
    "dataset = tf.data.Dataset.zip((user_data, product_data))\n",
    "\n",
    "# Lakukan batching dataset\n",
    "batch_size = 4096\n",
    "dataset = dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d4f96b08-4bdc-44e8-8bed-93317a064e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pastikan user_ids dan product_titles berupa string\n",
    "unique_user_ids = unique_user_ids.astype(str)\n",
    "unique_product_titles = unique_product_titles.astype(str)\n",
    "\n",
    "# Buat vocabulary untuk user IDs dan product titles\n",
    "user_ids_vocabulary = tf.keras.layers.StringLookup(vocabulary=unique_user_ids, mask_token=None)\n",
    "product_titles_vocabulary = tf.keras.layers.StringLookup(vocabulary=unique_product_titles, mask_token=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "092b87ec-97e7-4b7b-a884-3c97f68df31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Model\n",
    "user_model = tf.keras.Sequential([\n",
    "    user_ids_vocabulary,  # Map user IDs to integer indices\n",
    "    tf.keras.layers.Embedding(\n",
    "        user_ids_vocabulary.vocabulary_size(),\n",
    "        256,\n",
    "        embeddings_regularizer=tf.keras.regularizers.l2(1e-5)\n",
    "    )\n",
    "])\n",
    "\n",
    "# Product Model\n",
    "product_model = tf.keras.Sequential([\n",
    "    product_titles_vocabulary,  # Map product titles to integer indices\n",
    "    tf.keras.layers.Embedding(\n",
    "        product_titles_vocabulary.vocabulary_size(),\n",
    "        256,\n",
    "        embeddings_regularizer=tf.keras.regularizers.l2(1e-5)\n",
    "    )\n",
    "])\n",
    "\n",
    "task = tfrs.tasks.Retrieval(\n",
    "    metrics=tfrs.metrics.FactorizedTopK(\n",
    "        candidates=product.map(lambda x: x[\"title\"]).batch(128).map(product_model)  # Kandidat untuk retrieval\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "27d229eb-69fd-4fa0-ba9d-ae5a4ed6642d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional Model for User and Product\n",
    "def create_product_retrieval_model(user_model: tf.keras.Model, product_model: tf.keras.Model, task: tfrs.tasks.Retrieval):\n",
    "    # Input layers for user and product\n",
    "    user_input = tf.keras.Input(shape=(), dtype=tf.string, name=\"user_id\")\n",
    "    product_input = tf.keras.Input(shape=(), dtype=tf.string, name=\"product_title\")\n",
    "\n",
    "    # Embedding layers for user and product\n",
    "    user_embeddings = user_model(user_input)\n",
    "    product_embeddings = product_model(product_input)\n",
    "\n",
    "    # Calculate loss using task\n",
    "    loss = task(user_embeddings, product_embeddings)\n",
    "\n",
    "    # Define the model\n",
    "    model = tf.keras.Model(inputs=[user_input, product_input], outputs=loss)\n",
    "    return model\n",
    "\n",
    "# Contoh penggunaan\n",
    "model = create_product_retrieval_model(user_model, product_model, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166ffdd7-551b-4998-bb3d-d44637e2e2b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ce35ea18-7c52-4c8a-acf8-e1f619ad032d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Model.summary of <tf_keras.src.engine.functional.Functional object at 0x7f4e9075bbf0>>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='factorized_top_k/top_5_categorical_accuracy',\n",
    "    patience=500,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "fa51d377-84b2-40e5-bce8-d114c93d3dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user-labai-3/venv-master/lib/python3.12/site-packages/tf_keras/src/engine/functional.py:641: UserWarning: Input dict contained keys ['interaction_type'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/3000 - loss: 0.018226128071546555 - factorized_top_k/top_1_categorical_accuracy: 0.04600000008940697 - factorized_top_k/top_5_categorical_accuracy: 0.148499995470047 - factorized_top_k/top_10_categorical_accuracy: 0.26249998807907104 - factorized_top_k/top_50_categorical_accuracy: 0.9415000081062317 - factorized_top_k/top_100_categorical_accuracy: 1.0\n",
      "Epoch 200/3000 - loss: 0.0037649255245923996 - factorized_top_k/top_1_categorical_accuracy: 0.03799999877810478 - factorized_top_k/top_5_categorical_accuracy: 0.13899999856948853 - factorized_top_k/top_10_categorical_accuracy: 0.24650000035762787 - factorized_top_k/top_50_categorical_accuracy: 0.8774999976158142 - factorized_top_k/top_100_categorical_accuracy: 1.0\n",
      "Epoch 300/3000 - loss: 0.0006202742224559188 - factorized_top_k/top_1_categorical_accuracy: 0.03500000014901161 - factorized_top_k/top_5_categorical_accuracy: 0.12549999356269836 - factorized_top_k/top_10_categorical_accuracy: 0.24300000071525574 - factorized_top_k/top_50_categorical_accuracy: 0.809499979019165 - factorized_top_k/top_100_categorical_accuracy: 1.0\n",
      "Epoch 400/3000 - loss: 8.277344750240445e-05 - factorized_top_k/top_1_categorical_accuracy: 0.029500000178813934 - factorized_top_k/top_5_categorical_accuracy: 0.1264999955892563 - factorized_top_k/top_10_categorical_accuracy: 0.22699999809265137 - factorized_top_k/top_50_categorical_accuracy: 0.7360000014305115 - factorized_top_k/top_100_categorical_accuracy: 0.9580000042915344\n",
      "Epoch 500/3000 - loss: 8.904717105906457e-06 - factorized_top_k/top_1_categorical_accuracy: 0.03200000151991844 - factorized_top_k/top_5_categorical_accuracy: 0.11500000208616257 - factorized_top_k/top_10_categorical_accuracy: 0.21150000393390656 - factorized_top_k/top_50_categorical_accuracy: 0.675000011920929 - factorized_top_k/top_100_categorical_accuracy: 0.7179999947547913\n",
      "Epoch 559: early stopping\n",
      "Restoring model weights from the end of the best epoch: 59.\n"
     ]
    }
   ],
   "source": [
    "class LogEvery100Epochs(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % 100 == 0:  # Print every 100 epochs\n",
    "            metrics = \" - \".join([f\"{k}: {v}\" for k, v in logs.items()])\n",
    "            print(f\"Epoch {epoch + 1}/{self.params['epochs']} - {metrics}\")\n",
    "\n",
    "# Use the callback during training\n",
    "history = model.fit(\n",
    "    train_user.batch(4096),\n",
    "    epochs=3000,\n",
    "    callbacks=[LogEvery100Epochs(), early_stopping],\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "bc709032-dc05-40dd-a1cb-e92cb56c65c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Top-5 Accuracy: 0.3610\n",
      "Best Top-10 Accuracy: 0.3990\n",
      "Best Top-100 Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Access the best accuracy from the early stopping callback\n",
    "best_accuracy5 = history.history['factorized_top_k/top_5_categorical_accuracy'][early_stopping.best_epoch]\n",
    "best_accuracy10 = history.history['factorized_top_k/top_10_categorical_accuracy'][early_stopping.best_epoch]\n",
    "best_accuracy100 = history.history['factorized_top_k/top_100_categorical_accuracy'][early_stopping.best_epoch]\n",
    "print(f\"Best Top-5 Accuracy: {best_accuracy5:.4f}\")\n",
    "print(f\"Best Top-10 Accuracy: {best_accuracy10:.4f}\")\n",
    "print(f\"Best Top-100 Accuracy: {best_accuracy100:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "60e2bbea-a7a8-4757-8a59-04d565b39406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 recommended products for user 3:\n",
      "pecak ikan mujaer\n",
      "nila goreng cabe ijo\n",
      "ayam kecap bumbu giling\n",
      "beef sausage teriyaki w  salad\n",
      "sate buntel kambing\n"
     ]
    }
   ],
   "source": [
    "user_id_input = \"3\"  # Replace with an actual user ID\n",
    "k = 5  # The number of recommendations you want\n",
    "\n",
    "# Create the input tensor for the user ID\n",
    "user_input = tf.constant([user_id_input])\n",
    "\n",
    "# Get user embeddings from the trained user model\n",
    "user_embeddings = user_model(user_input)\n",
    "\n",
    "# Get all product titles and convert them into tensor format\n",
    "product_titles = product_titles_vocabulary.get_vocabulary()  # Get the list of product titles\n",
    "product_titles_tensor = tf.constant(product_titles)  # Convert product titles to tensor format\n",
    "\n",
    "# Use the product model to get product embeddings\n",
    "product_embeddings = product_model(product_titles_tensor)\n",
    "\n",
    "# Compute the cosine similarity or any other distance metric between the user and product embeddings\n",
    "# In this case, we use cosine similarity for simplicity\n",
    "similarity_scores = tf.linalg.matmul(user_embeddings, product_embeddings, transpose_b=True)\n",
    "\n",
    "# Get the top K products\n",
    "top_k_scores, top_k_indices = tf.math.top_k(similarity_scores, k=k)\n",
    "\n",
    "# Get the actual product titles for the top K indices\n",
    "top_k_product_titles = [product_titles[i] for i in top_k_indices.numpy()[0]]\n",
    "\n",
    "print(f\"Top {k} recommended products for user {user_id_input}:\")\n",
    "for title in top_k_product_titles:\n",
    "    print(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "02cb97d4-61dd-4f02-b3d3-519886639753",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[157], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel.keras\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv-master/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/venv-master/lib/python3.12/site-packages/tf_keras/src/saving/legacy/save.py:151\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    143\u001b[0m     save_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (h5py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath, h5py\u001b[38;5;241m.\u001b[39mFile))\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m saving_utils\u001b[38;5;241m.\u001b[39mis_hdf5_filepath(filepath)\n\u001b[1;32m    146\u001b[0m ):\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;66;03m# TODO(b/130258301): add utility method for detecting model type.\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model\u001b[38;5;241m.\u001b[39m_is_graph_network \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    149\u001b[0m         model, sequential\u001b[38;5;241m.\u001b[39mSequential\n\u001b[1;32m    150\u001b[0m     ):\n\u001b[0;32m--> 151\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    152\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving the model to HDF5 format requires the model to be a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunctional model or a Sequential model. It does not work for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubclassed models, because such models are defined via the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody of a Python method, which isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt safely serializable. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsider saving to the Tensorflow SavedModel format (by \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msetting save_format=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m) or using `save_weights`.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    158\u001b[0m         )\n\u001b[1;32m    159\u001b[0m     hdf5_format\u001b[38;5;241m.\u001b[39msave_model_to_hdf5(\n\u001b[1;32m    160\u001b[0m         model, filepath, overwrite, include_optimizer\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`."
     ]
    }
   ],
   "source": [
    "model.save('model.keras', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "890a2ac7-394a-422d-886b-f93d9b4bc777",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model_weights.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "master"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
